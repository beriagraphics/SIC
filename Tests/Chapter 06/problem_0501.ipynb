{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Quiz #0501"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### \"Logistic Regression and Gradient Descent Algorithm\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Answer the following questions by providing Python code:\n",
    "#### Objectives:\n",
    "- Code a logistic regression class using only the NumPy library.\n",
    "- Implement in Python the Sigmoid function.\n",
    "- Implement in Python the Gradient of the logarithmic likelihood.\n",
    "- Implement in Python the Gradient Descent Algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from sklearn.datasets import load_breast_cancer\r\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Read in data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Load data.\r\n",
    "data = load_breast_cancer()\r\n",
    "# Explanatory variables.\r\n",
    "X = data['data']\r\n",
    "# Relabel such that 0 = 'benign' and 1 = malignant.\r\n",
    "Y = 1 - data['target']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Split the dataset into training and testing.\r\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=1234)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1). Define the 'sigmoid' and 'gradient' functions to produce the output shown below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "import scipy.stats as st\r\n",
    "def sigmoid(x):\r\n",
    "       # <Your code goes in here>\r\n",
    "       return 1/ (1 + 1/np.exp(x))\r\n",
    "\r\n",
    "def gradient(X, Y, beta, bias):\r\n",
    "       # <Your code goes in here>\r\n",
    "       N = X.shape[0]\r\n",
    "       z = np.dot(X, beta) + bias\r\n",
    "       Y_pred = sigmoid(z)\r\n",
    "       \r\n",
    "       cost=-(1/N)*np.sum(Y*np.log(Y_pred)+(1-Y)*np.log(1-Y_pred))\r\n",
    "       \r\n",
    "       da=1/N*np.dot(X.T,(Y_pred-Y).T)\r\n",
    "       db=1/N*np.sum(Y_pred-Y)\r\n",
    "\r\n",
    "       return (cost, db, da)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2). Define the 'LogisticRegression' class to produce the output shown below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "class LogisticRegression:\r\n",
    "    def __init__(self, learn_rate):\r\n",
    "      # <Your code goes in here>\r\n",
    "      self.costs = []\r\n",
    "      self.learn_rate = learn_rate\r\n",
    "      self.n_iter = 100\r\n",
    "\r\n",
    "    def train(self, input_X, input_Y, n_epochs):\r\n",
    "      # <Your code goes in here>\r\n",
    "      # Randomly Initialize Slope and Intercept\r\n",
    "      self.b = np.random.randint(0, 5) #random Intercept\r\n",
    "      self.a = []\r\n",
    "      for i in range(input_X.shape[1]): #random slope initialization\r\n",
    "          self.a.append([np.random.randint(1, 5)])\r\n",
    "      # print(self.a)\r\n",
    "      # For education purposes, we will use constant values (delete this during finilization)\r\n",
    "      # self.b = 0  #Intercept \r\n",
    "      # self.a = np.ones((input_X.shape[1], 1)) #Slope\r\n",
    "\r\n",
    "      \r\n",
    "      # Let's implement mini-batches/stocastic \r\n",
    "      for e in range(n_epochs):\r\n",
    "        for i in range(self.n_iter):\r\n",
    "          #  if self.batch_size == None:\r\n",
    "          #        self.batch_size = input_X.shape[0]\r\n",
    "          \r\n",
    "          #  it = int(input_X.shape[0]/self.batch_size)   # number of mini-batches. Equals len(input_X) if Stocastic GD\r\n",
    "          # for j in range(it):\r\n",
    "          # Splitting the Input/Target by batch_size\r\n",
    "          #  X = input_X[self.batch_size*j: self.batch_size*(j+1)]\r\n",
    "          #  Y = input_Y[self.batch_size*j: self.batch_size*(j+1)]\r\n",
    "\r\n",
    "          # Calculate the predicted values using the coeficients\r\n",
    "          \r\n",
    "          cost, db, da = gradient(input_X, input_Y, self.a, self.b)\r\n",
    "          # print(N)\r\n",
    "          # The cost function\r\n",
    "          # cost=-(1/N)*np.sum(input_Y*np.log(Y_pred)+(1-input_Y)*np.log(1-Y_pred))\r\n",
    "          # self.loss.append(sum(pow(input_Y - Y_pred, 2))[0])\r\n",
    "          # da=1/N*np.dot(input_X.T,(Y_pred-input_Y).T)\r\n",
    "          # db=1/N*np.sum(Y_pred-input_Y)\r\n",
    "\r\n",
    "          # Derivate with respect to Bias (Intercept)\r\n",
    "          # db = sum(-2*(input_Y - Y_pred))\r\n",
    "          \r\n",
    "          # da = []        \r\n",
    "          # for j in range(len(self.a)):\r\n",
    "          #   X1 = input_X[:, j].reshape(-1, 1) # Feature seperation\r\n",
    "          #   # Derivate with respect to Slope\r\n",
    "          #   da.append( sum(-2*X1*(input_Y - Y_pred)).tolist() )\r\n",
    "            \r\n",
    "          # updating slope/ intercept\r\n",
    "          self.a = self.a - self.learn_rate*da\r\n",
    "          self.b = self.b - self.learn_rate*db\r\n",
    "\r\n",
    "          #Records cost\r\n",
    "        # self.costs.append(cost)\r\n",
    "        # print(cost)\r\n",
    "\r\n",
    "\r\n",
    "    # def query(self, input_X, prob=True, cutoff=0.5):\r\n",
    "\r\n",
    "      # <Your code goes in here>"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sample run:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# Hyperparameter for the learner.\r\n",
    "learning_rate = 0.001"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# Train and predict.\r\n",
    "LR = LogisticRegression(learning_rate)\r\n",
    "LR.train(X_train, Y_train, 5)\r\n",
    "# Y_pred = LR.query(X_test,prob=False,cutoff=0.5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-4-ebe8138d3bed>:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/ (1 + np.exp(-x))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Display the accuracy.\r\n",
    "acc = (Y_pred == Y_test.reshape(-1,1)).mean()\r\n",
    "print('Accuracy : {}'.format(np.round(acc,3)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 0.912\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# import matplotlib.pyplot as plt\r\n",
    "# plt.plot(LR.costs)\r\n",
    "LR.costs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[nan, nan, nan, nan, nan]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (system)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "interpreter": {
   "hash": "d373a6adc26a229c2c7b2016ce264c6636cd7db4cb097b04765bf225c02b830d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}